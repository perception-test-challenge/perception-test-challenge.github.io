<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <title>Interpretability Track - The Third Perception Test Challenge at ICCV 2025</title>
  <meta content="width=device-width, initial-scale=1.0" name="viewport">
  <meta content="" name="keywords">
  <meta name="description" content="Details for the Interpretability track of the 3rd Perception Test Challenge at ICCV 2025.">

  <!-- Favicons -->
  <link href="assets/fav.png" rel="icon">
  <link href="assets/fav.png" rel="apple-touch-icon">
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600&display=swap">
  <style>
    body {
      font-family: 'Inter', sans-serif;
      background-color: #fefefe;
      color: #333;
      margin: 0;
      padding: 0;
    }

    .banner-wrapper {
      max-width: 900px;
      margin: 0 auto;
      padding: 1rem;
    }

    .banner {
      max-width: 60%;
      height: auto;
      display: block;
      margin: 0 auto 2rem;
    }

    .section {
      max-width: 900px;
      margin: 3rem auto;
      padding: 0 1rem;
    }

    h1, h2, h3 {
      text-align: center;
      color: #202124;
    }

    a {
      color: #1a73e8; /* Blue color */
      text-decoration: none; /* Removes the default underline */
      font-weight: bold;
    }

    footer {
      text-align: center;
      margin-top: 3rem;
      padding: 2rem;
      background: #f5f5f5;
      font-size: 0.9rem;
    }
    
    .content-section {
        text-align: left;
        margin-bottom: 2rem;
    }
    
    .content-section h3 {
        text-align: left;
        color: #202124;
        border-bottom: 2px solid #f1f3f4;
        padding-bottom: 0.5rem;
        margin-top: 2rem;
    }

    ul {
        list-style-type: disc;
        padding-left: 20px;
    }

    .button-row {
      display: flex;
      justify-content: center;
      flex-wrap: wrap;
      gap: 1rem;
      margin: 2rem 0;
    }

    .btn {
      background: #1a73e8;
      color: white;
      padding: 0.75rem 1.5rem;
      border-radius: 9999px;
      text-decoration: none;
      font-weight: 600;
      transition: background 0.2s ease;
    }

    .btn:hover {
      background: #155ec0;
    }

    .nice-table {
      width: 100%;
      border-collapse: collapse; /* Collapses the borders into a single border */
      margin: 25px 0;
      font-size: 0.9em;
      font-family: sans-serif;
      min-width: 400px;
      box-shadow: 0 0 20px rgba(0, 0, 0, 0.15);
    }

    .nice-table thead tr {
      <!-- background-color: #4e70fa; -->
      background-color: #f1f1f1;
      <!-- color: #ffffff; -->
      text-align: left;
    }

    .nice-table th,
    .nice-table td {
      padding: 12px 15px;
    }

    .nice-table tbody tr {
      border-bottom: 1px solid #dddddd;
    }

    .nice-table tbody tr:nth-of-type(even) {
      background-color: #f3f3f3; /* Zebra-striping for even rows */
    }

    .nice-table tbody tr:hover {
      background-color: #f1f1f1; /* Highlight row on hover */
    }
  </style>
</head>
<body>
  <h1 style="position:absolute; left:-9999px; top:auto; width:1px; height:1px; overflow:hidden;">
    Interpretability Track - The 3rd Perception Test Challenge ICCV 2025
  </h1>

  <div class="section">
    <h1>Interpretability Track</h1>
    <p style="text-align:center;">Use Perception Test as a benchmark for VLM interpretability and win prizes!</p>

  <!-- <div class="banner-wrapper">
    <a href="index.html"><img class="banner" src="bannerPT.png" alt="The 3rd Perception Test Challenge Banner"></a>
  </div> -->
    <figure style="margin: 0;">
    <video autoplay loop muted playsinline style="max-width: 100%; width: 100%; border-radius: 8px; margin-top: 1rem; display: block;">
      <source src="assets/attn_overlay_numcups_f4_2x2.mp4" type="video/mp4">
      Your browser does not support the video tag.
    </video>
    <figcaption style="text-align: center; padding: 8px; font-style: italic;">
      Visualization of the attention maps of PerceptionLM with four video frames as input overlaid with ground truth object tracks.
      Only high activity layers are shown.
      
      The task is: "The person uses multiple similar objects to play an occlusion game. How many such objects does the person use?"
    </figcaption>
    </figure>


    <div class="content-section">
        <h3>Overview</h3>
        <p>The goal of the Interpretability track is to encourage the development of techniques that provide insights into how state-of-the-art perception models make their decisions. 
          Methods can be varied in nature, i.e. behavioral/black-box, mechanistic, or simply visualizations that explain why a model succeeds or fails at one or more Perception Test tasks.</p>  

         <h3>How to participate</h3>
          <p>You can use any method you prefer, as long as it highlights convincingly how a model solves (or fails at solving) one or more tasks in the Perception Test.</p>
          <p>You will be asked to submit a colab notebook demonstrating your explanations or predictions on example videos from the Perception Test benchmark, as well as a short tech report (max 2 pages) decribing the analysed models, how they were initially trained, and the methods used for interpretability and analysis.</p>
        <p>Bonus points for works that leverage the different types of annotations in the Perception Test to design quantitative explainability methods, e.g. show correlations between saliency / attention maps produced when answering videoQAs and ground truth object tracks.</p>
    </div>

    <div class="content-section">
        <h3>Perception Test tasks</h3>
        <p>In Perception Test, there are 132 unique questions in the multiple-choice videoQA dataset. Each question is applied to multiple videos (from 20 videos up to more than 1000 videos). We define as <i>Perception Test task</i> a videoQA and the videos it is applied to across train / valid / test splits of the benchmark.</p>
        A meaningful interpretability method should identify a pattern or explanation that applies to all or most of the videos within one or more tasks. </p>

        <h4>Example of a <i>Perception Test task</i></h4> 
          <p>
          Question: <i>"The person uses multiple similar objects to play an occlusion game. How many such objects does the person use?"</i></p>
          <p>Options: a) <i>4</i> b) <i>2</i> c) <i>3</i> </p>
          For this task, there are 116 (out of 2184) train, 305 (out of 5900) valid, and 189 (out of 3525) test videos; see example below. These videos also have ground truth annotations for object tracks, action segments, and sound segments that can be used to quantitatively prove that the model is paying attention to the relevant spatio-temporal regions in the frames.</p>
      <p>
      <figure style="margin: 0;">
        <img src="assets/numcups_attn_box1.png" alt="Maximum activations over layers" style="max-width: 100%; width: 100%; border-radius: 8px; margin-top: 1rem;">
        <figcaption style="text-align: center; padding: 8px; font-style: italic;">
          Maximum activations over all layers and heads. We can see the attention has high overlap with the objects in the first and last frames (as shown by the boxes).
        </figcaption>
      </figure>
      </p>
      
      <h3>Getting Started with Perception Test</h3>
      <p>To get started with Perception Test, our github page has all of the relevant information. Below is a list of pointers to useful information:</p>
      <!-- <ul>
        <li>
          <a href="https://github.com/google-deepmind/perception_test">Perception Test Github page</a>
        </li>
      </ul> -->

      <table class="nice-table">
  <!-- <caption>Employee Information</caption> -->
  <!-- <thead>
    <tr>
      <th>Name</th>
      <th>Position</th>
      <th>Email</th>
      <th>Location</th>
    </tr>
  </thead> -->
  <tbody>
    <tr>
      <td><b>Getting Started</b></td>
      <td><a href="https://github.com/google-deepmind/perception_test">Perception Test Github Page</a> </td> 
      <td>Demo: <a href="https://github.com/deepmind/perception_test/blob/main/data_visualisation.ipynb">data_visualisation.ipynb</a> </td> 
      <td>Interp Demo: <a href="https://github.com/ptchallenge-workshop/TransformerLens/blob/master/demos/PerceptionLM_Demo.ipynb">PerceptionLM_Demo.ipynb</a> </td> 
    </tr>
    <tr>
      <td><b>Sample Split</b></td>
      <td><a href="https://storage.googleapis.com/dm-perception-test/zip_data/sample_videos.zip">sample_videos.zip (215MB)</a></td>
      <td> <a href="https://storage.googleapis.com/dm-perception-test/zip_data/sample_audios.zip">sample_audios.zip (84MB)</a></td>
      <td><a href="https://storage.googleapis.com/dm-perception-test/zip_data/sample_annotations.zip">sample_annotations.zip (3MB)</a></td>
    </tr>
    <tr>
      <td><b>Train Split</b></td>
      <td><a href="https://storage.googleapis.com/dm-perception-test/zip_data/train_videos.zip">train_videos.zip (26.5GB)</a></td>
      <td> <a href="https://storage.googleapis.com/dm-perception-test/zip_data/train_audios.zip">train_audios.zip (12.3GB) </a></td>
      <td><a href="https://storage.googleapis.com/dm-perception-test/zip_data/train_annotations.zip">train_annotations.zip (30.6MB)</a></td>
    </tr>
    <tr>
      <td><b>Valid Split</b></td>
      <td><a href="https://storage.googleapis.com/dm-perception-test/zip_data/valid_videos.zip">valid_videos.zip (70.2GB)</a></td>
      <td> <a href="https://storage.googleapis.com/dm-perception-test/zip_data/valid_audios.zip">valid_audios.zip (33.1MB)</a></td>
      <td><a href="https://storage.googleapis.com/dm-perception-test/zip_data/valid_annotations.zip">valid_annotations.zip (81.5MB)</a></td>
    </tr>
    <tr>
      <td><b>Test Split</b></td>
      <td><a href="https://storage.googleapis.com/dm-perception-test/zip_data/test_videos.zip">test_videos.zip (41.8GB)</a></td>
      <td> <a href="https://storage.googleapis.com/dm-perception-test/zip_data/test_audios.zip">test_audios.zip (19.3GB)</a></td>
      <td><a href="https://storage.googleapis.com/dm-perception-test/zip_data/test_annotations.zip">test_annotations.zip (633.9kB)</a></td>
    </tr>
    <tr>
      <td><b>Cut Frames</b></td>
      <td><a href="https://storage.googleapis.com/dm-perception-test/misc/cut_frame_mapping_train.json">cut_frame_mapping_train.json</a></td>
      <td><a href="https://storage.googleapis.com/dm-perception-test/misc/cut_frame_mapping_valid.json">cut_frame_mapping_valid.json</a></td>
      <td>For videos where the end gives away the answer (cup-games, stability, etc.)</td>
    </tr>

    <tr>
      <td>Annotation Details</td>
      <td><a href="https://github.com/google-deepmind/perception_test?tab=readme-ov-file#perception-test-annotations">Descriptions</a> on Github.</td>
      <!-- <td>john.doe@example.com</td> -->
    </tr>

  </tbody>
</table>

    <!-- <div class="content-section">  -->

      <h3>Resources and Examples</h3>
      <p>We provide a starter kit with examples for generating visualizations on videos from the Perception Test, as well as some reference works that study LLMs or image VLMs, which might be applicable to video VLMs.
      
      <p>Featured at the top of this page is an example of a visualization using the ground truth object tracks overlaid with the visual attention from PerceptionLM using 4 frames as input.
          At intermediate layers, we can see where the model pays particular attention to (some of the cups, but also other areas!):
      </p>

      <p>The figure above shows the maximum activations over all the layers overlaid on the video frames.</p>

      You can find this visualization in the PerceptionLM demo notebook linked below, which uses TransformerLens. 
    
      <p></p>

        Suggested resources and ideas to explore (please ignore if not useful for your approach):
        <ul>
            <li>
              <a href="https://github.com/ptchallenge-workshop/TransformerLens/blob/master/demos/PerceptionLM_Demo.ipynb"
                   target="_blank">PerceptionLM tutorial</a> in PyTorch using Transformer Lens for visualizing attention maps overlaid on object bounding box tracks.
                <!-- <li><a href="#" target="_blank">Gemma 3 tutorial</a> in Jax using Penzai</li>  -->
                <!-- <li>Gemma 3 tutorial in Jax using Penzai (coming soon)</li>  -->
            <li>
              <p>
              Predict the accuracy of the model without labels, perhaps from some other signals in the model, e.g. confidence. 
              This <a href="https://arxiv.org/abs/2305.14802">paper</a> tries to do so for LLMs by training a meta-model on top of confidence scores.
              </p>
            </li>
            <li>
              <p>  
              Use logit lens to analyze object hallucinations. E.g. These papers (<a href="https://arxiv.org/abs/2410.02762">[1]</a>, 
              <a href="https://arxiv.org/abs/2411.16724">[2]</a>) analyse image VLMs, attributing hallucinations to low confidence logit readouts and poor visual attention activity in early layers. 
              </p>
            </li>
            <li>
              <p>Analyse the geometric distribution 
                of attention as an indicator for how well models can do spatial reasoning; e.g. this paper (<a href="https://arxiv.org/abs/2503.01773">[3]</a>) for image VLMs. 
              </p>
            </li>
        </ul>
    </div>

    <div class="content-section">
        <h3>Judging Criteria and Timeline</h3>
        <p>Submissions will be evaluated single-blindedly by the organising team, considering the below criteria: </p>
          <ul>
            <li><b>Generality</b>: How convincingly does the method explain the model behaviour across all videos in one or more Perception Test tasks? Does the method provide qualitative or quantitative results?</li>
            <li><b>Novelty</b>: Does the proposed method bring new insights into understanding the strengths and failures of video VLMs?</li>
            <li><b>Clarity and reproducibility</b>: Are the methods and results clearly described in the tech report and can they be easily reproduced when running the colab?</li>
            <!-- <li>Interpretability </li> -->
          </ul>
        <p>The timeline for this track follows the main challenge:</p>
        <ul>
          <li><b>September 1st</b>: Interpretability track open for submissions</li>
          <li><b>October 6</b>: Deadline for colab submissions (11:59pm AOE)</li>
          <li><b>October 8</b>: Deadline for report submissions (11:59pm AOE)</li>
          <li><b>October 10</b>: Decision to participants</li>
        </ul>
        <p>Upload your final colab notebook file and tech report to this <a href="https://docs.google.com/forms/d/e/1FAIpQLSdrDwnjrqQ568y7cJBG8CGgh77t9o3YTFQyDOJ3FgGu7I2yAw/viewform?usp=header" target="_blank">Google Form</a>.</p>

        <p>For report submissions which come in after the colab submissions (to give people a few extra days to work on the reports), edit your original form submission with your report once it's ready.</p>
    </div>

    <div class="button-row">
        <a href="index.html" class="btn">Back to Main Page</a>
    </div>
  </div>

  <footer>
    &copy; 2025 Perception Test Team — All Rights Reserved
  </footer>
</body>
</html>
