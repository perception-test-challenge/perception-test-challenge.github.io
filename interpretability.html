<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <title>Interpretability Track - The Third Perception Test Challenge at ICCV 2025</title>
  <meta content="width=device-width, initial-scale=1.0" name="viewport">
  <meta content="" name="keywords">
  <meta name="description" content="Details for the Interpretability track of the 3rd Perception Test Challenge at ICCV 2025.">

  <!-- Favicons -->
  <link href="assets/fav.png" rel="icon">
  <link href="assets/fav.png" rel="apple-touch-icon">
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600&display=swap">
  <style>
    body {
      font-family: 'Inter', sans-serif;
      background-color: #fefefe;
      color: #333;
      margin: 0;
      padding: 0;
    }

    .banner-wrapper {
      max-width: 900px;
      margin: 0 auto;
      padding: 1rem;
    }

    .banner {
      max-width: 60%;
      height: auto;
      display: block;
      margin: 0 auto 2rem;
    }

    .section {
      max-width: 900px;
      margin: 3rem auto;
      padding: 0 1rem;
    }

    h1, h2, h3 {
      text-align: center;
      color: #202124;
    }

    a {
      color: #1a73e8; /* Blue color */
      text-decoration: none; /* Removes the default underline */
      font-weight: bold;
    }

    footer {
      text-align: center;
      margin-top: 3rem;
      padding: 2rem;
      background: #f5f5f5;
      font-size: 0.9rem;
    }
    
    .content-section {
        text-align: left;
        margin-bottom: 2rem;
    }
    
    .content-section h3 {
        text-align: left;
        color: #202124;
        border-bottom: 2px solid #f1f3f4;
        padding-bottom: 0.5rem;
        margin-top: 2rem;
    }

    ul {
        list-style-type: disc;
        padding-left: 20px;
    }

    .button-row {
      display: flex;
      justify-content: center;
      flex-wrap: wrap;
      gap: 1rem;
      margin: 2rem 0;
    }

    .btn {
      background: #1a73e8;
      color: white;
      padding: 0.75rem 1.5rem;
      border-radius: 9999px;
      text-decoration: none;
      font-weight: 600;
      transition: background 0.2s ease;
    }

    .btn:hover {
      background: #155ec0;
    }
  </style>
</head>
<body>
  <h1 style="position:absolute; left:-9999px; top:auto; width:1px; height:1px; overflow:hidden;">
    Interpretability Track - The 3rd Perception Test Challenge ICCV 2025
  </h1>

  <div class="section">
    <h1>Interpretability Track</h1>
    <p style="text-align:center;">Use Perception Test as a benchmark for VLM interpretability and win prizes!</p>

  <!-- <div class="banner-wrapper">
    <a href="index.html"><img class="banner" src="bannerPT.png" alt="The 3rd Perception Test Challenge Banner"></a>
  </div> -->
    <video autoplay loop muted playsinline style="max-width: 100%; width: 100%; border-radius: 8px; margin-top: 1rem;">
        <source src="assets/attn_overlay_numcups_f4_2x2.mp4" type="video/mp4">
        Your browser does not support the video tag.
    </video>

    <div class="content-section">
        <h3>Overview</h3>
        <p>The goal of the Interpretability track is to encourage the development of techniques that provide insights into how state-of-the-art perception models make their decisions. 
          Methods can be varied in nature, i.e. behavioral/black-box, mechanistic, or simply visualizations that explain why a model succeeds or fails at one or more Perception Test tasks.</p>  

         <h3>How to participate</h3>
          
          <p>Participants will be asked to submit a colab notebook demonstrating their explanations or predictions on example videos from the Perception Test benchmark, as well as a short tech report (max 2 pages).</p>
        <p>Bonus points for works that leverage the different types of annotations in the Perception Test to design quantitative explainability methods, e.g. show correlations between saliency / attention maps produced when answering videoQAs and ground truth object tracks.</p>
    </div>

    <div class="content-section">
        <h3>Resources and Examples</h3>
        <p>You can use any method you prefer, as long as it highlights convincingly how a model solves (or fails at solving) some task in the Perception Test. We provide a starter kit with examples for generating visualizations on videos from the Perception Test, as well as some reference papers for possible directions to investigate.
        </p>
        <h4>Choosing a <i>Perception Test task</i></h4>
        <p>In Perception Test, there are 132 unique questions in the multiple-choice videoQA dataset. Each question is applied to multiple videos (from 20 videos up to more than 1000 videos). We define as <i>Perception Test task</i> a videoQA and the videos it is applied to across train / valid / test splits of the benchmark.</p>
        A meaningful interpretability method should identify a pattern or explanation that applies to all videos within one or more tasks. </p>

        <h4>Example of a <i>Perception Test task</i></h4>
        <p> As an example, one focus could be counting cups in cup-game videos, which are identified by the question 
          <p>
          <i>"The person uses multiple similar objects to play an occlusion game. How many such objects does the person use?"</i>.</p>
          There are 116 (out of 2184) train, 305 (out of 5900) valid, and 189 (out of 3525) test videos; see example below.</p>
      <p>These videos also have ground truth annotations for object tracks, action segments, and sound segments that can be used to prove that the model is paying attention to the relevant spatio-temporal regions in the frames.</p>

      <p>Featured at the top of this page is an example of a visualization using the ground truth object tracks overlaid with the visual attention from PerceptionLM using 4 frames as input.
          At intermediate layers, we can see where the model pays particular attention to (some of the cups, but also other areas!):
      </p>

      <p>Here is what the maximum activation over all of the layers looks like overlaid on the video frames.
      <img src="assets/numcups_attn_box.png" alt="Maximum activations over layers" style="max-width: 100%; width: 100%; border-radius: 8px; margin-top: 1rem;">
      </p>
      You can find this visualization in the PerceptionLM demo notebook linked above.
      An immediate interesting question which has been tackled before in vision but not for video is to better understand how this attention is geometrically distributed
      with respect to the bounding boxes.
      This notebook provides a jump start for those interested in this type of analysis, as well as others enabled by the powerful flexibility of TransformerLens. 
    
      <p></p>

        Some suggested resources ideas, to directy your investigation but are by no means necessary to follow:
        <ul>
            <li>
              <a href="https://github.com/ptchallenge-workshop/TransformerLens/blob/master/demos/PerceptionLM_Demo.ipynb"
                   target="_blank">PerceptionLM tutorial</a> in PyTorch using Transformer Lens for visualizing attention maps overlaid on object bounding box tracks.
                <!-- <li><a href="#" target="_blank">Gemma 3 tutorial</a> in Jax using Penzai</li>  -->
                <!-- <li>Gemma 3 tutorial in Jax using Penzai (coming soon)</li>  -->
            <li>
              <p>
              Can we predict the accuracy of our model without labels, perhaps from some other signals in the video? 
              This <a href="https://arxiv.org/abs/2305.14802">paper</a> tries to do so for LLMs by training a meta-model on top of confidence scores.
              </p>
            </li>
            <li>
              <p>
              How well can mechanistic interpretability tools be used VLMs, and by extension Video-Language Models (VidLMs)? 
              There are existing papers which use logit lens (<a href="https://arxiv.org/abs/2410.02762">[1]</a>, 
              <a href="https://arxiv.org/abs/2411.16724">[2]</a>) to analyze object hallucinations, tracking them 
              to low confidence logit readouts and poor visual attention activity in early layers. 
              How do we extend these to VidLMs, and how to we aggregate the information across frames?
              </p>
            </li>
            <li>
              <p>
                Other papers (<a href="https://arxiv.org/abs/2503.01773">[3]</a>) have looked at the geometric distribution 
                of attention in the image as an indicator for how well LLaVA models can do spatial reasoning.
                Can we use our ground truth object bounding labels to also analyze how informative these attention maps are for 
                accurate video reasoning?
              </p>
            </li>
        </ul>
    </div>

    <div class="content-section">
        <h3>Judging Criteria and Timeline</h3>
        <p>Submissions will be evaluated single-blind by the organising team, and the best submissions will receive prizes. 
          Ideal submissions will have the following characteristics:</p>
          <ul>
            <li><b>Novelty for Videos</b>: Very few techniques have been shown on videos for understanding model behaviors better, so any new additions will be interesting. </li>
            <li><b>Generality</b>: Some techniques will be particularly specific to subsets of tasks. Good techniques will be applicable to many.</li>
            <li><b>Resourcefulness</b>: We have a lot of annotations provided in the Perception Test dataset, from object and point tracks to grounded VQA. 
              While we are attempting to unify these this year, there can be more in-depth analysis using them to better understand model behavior.</li>
            <!-- <li>Interpretability </li> -->
          </ul>
        <p>The timeline for this track follows the main challenge:</p>
        <ul>
          <li><b>September 1st</b>: Interpretability track open for submissions</li>
          <li><b>October 6</b>: Deadline for submissions (11:59pm AOE)</li>
          <li><b>October 10</b>: Decision to participants</li>
        </ul>
        <p>Upload your final colab notebook files and short tech reports to this Google Form (TODO: add unified form).</p>
    </div>

    <div class="button-row">
        <a href="index.html" class="btn">Back to Main Page</a>
    </div>
  </div>

  <footer>
    &copy; 2025 Perception Test Team â€” All Rights Reserved
  </footer>
</body>
</html>
